# Machine-Learning-Group

## 文献个人批注汇总

> 颜色说明：
>
> - 论文名：蓝色加粗
> - 批注：正文颜色
> - 期刊信息：灰色斜体（仅显示存在的子字段）

---

<p><span style="color:#1f618d; font-weight:600">Data-driven stock forecasting models based on neural networks: A review：</span><br>
批注：关于数据驱动的股票预测，不同模型的对比分析综述；3.6节，基于 transformer 的预测模型。<br>
<span style="color:#566573; font-style:italic">期刊信息（影响因子: 15.5；JCR分区: Q1；中科院分区: 计算机科学1区）</span></p>

---

<p><span style="color:#1f618d; font-weight:600">An improved BERT method for the evolution of network public opinion of major infectious diseases: Case study of COVID-19：</span><br>
批注：PCA-BERT，用于融合层中信息。<br>
<span style="color:#566573; font-style:italic">期刊信息（影响因子: 7.5；JCR分区: Q1；中科院分区: 计算机科学1区）</span></p>

---

<p><span style="color:#1f618d; font-weight:600">BERT-CNN: Improving BERT for requirements classification using CNN：</span><br>
批注：2020 年于会议提出 BERT-CNN 架构。</p>

---

<p><span style="color:#1f618d; font-weight:600">Recent advances in natural language processing via large pre-trained language models: A survey：</span><br>
批注：NLP 领域综述。<br>
<span style="color:#566573; font-style:italic">期刊信息（影响因子: 28.0；JCR分区: Q1；中科院分区: 计算机科学1区）</span></p>

---

<p><span style="color:#1f618d; font-weight:600">A primer in BERTology: What we know about how BERT works：</span><br>
批注：文中明确提到，目前已经提出了中间层深层信息的需要被利用，但提到的研究只有类似加权融合。<br>
<span style="color:#566573; font-style:italic">期刊信息（影响因子: 6.9；JCR分区: Q1；中科院分区: 计算机科学2区；CCF: B）</span></p>

---

<p><span style="color:#1f618d; font-weight:600">The automatic text classification method based on BERT and feature union：</span><br>
批注：首个关于 CNN 等与 BERT 结合的论文。<br>
<span style="color:#566573; font-style:italic">期刊信息（CCF: C）</span></p>

---

<p><span style="color:#1f618d; font-weight:600">BERT-JAM: Maximizing the utilization of BERT for neural machine translation：</span><br>
批注：明确提到，只观察最后一层的量会导致信息流失，但只对层进行了简单融合。<br>
<span style="color:#566573; font-style:italic">期刊信息（影响因子: 6.5；JCR分区: Q1；中科院分区: 计算机科学2区；CCF: C）</span></p>

---

<p><span style="color:#1f618d; font-weight:600">BERT: Pre-training of deep bidirectional transformers for language understanding：</span><br>
批注：2019 - BERT 开山之作。<br>
<span style="color:#566573; font-style:italic">期刊信息（CCF: B）</span></p>
