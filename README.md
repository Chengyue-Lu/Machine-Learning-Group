# Machine-Learning-Group

## 文献个人批注汇总

+ Data-driven stock forecasting models based on neural networks: A review：
批注：关于数据驱动的股票预测，不同模型的对比分析综述；3.6节，基于 transformer 的预测模型。
期刊信息（影响因子: 15.5；JCR分区: Q1；中科院分区: 计算机科学1区）

+ An improved BERT method for the evolution of network public opinion of major infectious diseases: Case study of COVID-19：
批注：PCA-BERT，用于融合层中信息。
期刊信息（影响因子: 7.5；JCR分区: Q1；中科院分区: 计算机科学1区）

+ BERT-CNN: Improving BERT for requirements classification using CNN：
批注：2020 年于会议提出 BERT-CNN 架构。

+ Recent advances in natural language processing via large pre-trained language models: A survey：
批注：NLP 领域综述。
期刊信息（影响因子: 28.0；JCR分区: Q1；中科院分区: 计算机科学1区）

+ A primer in BERTology: What we know about how BERT works：
批注：文中明确提到，目前已经提出了中间层深层信息的需要被利用，但提到的研究只有类似加权融合。
期刊信息（影响因子: 6.9；JCR分区: Q1；中科院分区: 计算机科学2区；CCF: B）

+ The automatic text classification method based on BERT and feature union：
批注：首个关于 CNN 等与 BERT 结合的论文。
期刊信息（CCF: C）

+ BERT-JAM: Maximizing the utilization of BERT for neural machine translation：
批注：明确提到，只观察最后一层的量会导致信息流失，但只对层进行了简单融合。
期刊信息（影响因子: 6.5；JCR分区: Q1；中科院分区: 计算机科学2区；CCF: C）

+ BERT: Pre-training of deep bidirectional transformers for language understanding：
批注：2019 - BERT 开山之作。
期刊信息（CCF: B）

## 关于第一次汇报

+ 汇报相关的大纲脉络已经在pre 1st文件夹下，有问题随时咨询
