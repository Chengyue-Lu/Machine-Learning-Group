
@article{bao_data-driven_2025,
	title = {Data-driven stock forecasting models based on neural networks: {A} review},
	volume = {113},
	issn = {15662535},
	shorttitle = {Data-driven stock forecasting models based on neural networks},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253524003944},
	doi = {10.1016/j.inffus.2024.102616},
	abstract = {As a core branch of financial forecasting, stock forecasting plays a crucial role for financial analysts, investors, and policymakers in managing risks and optimizing investment strategies, significantly enhancing the efficiency and effectiveness of economic decision-making. With the rapid development of information technology and computer science, data-driven neural network technologies have increasingly become the mainstream method for stock forecasting. Although recent review studies have provided a basic introduction to deep learning methods, they still lack detailed discussion on network architecture design and innovative details. Additionally, the latest research on emerging large language models and neural network structures has yet to be included in existing review literature. In light of this, this paper comprehensively reviews the literature on datadriven neural networks in the field of stock forecasting from 2015 to 2023, discussing various classic and innovative neural network structures, including Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), Transformers, Graph Neural Networks (GNNs), Generative Adversarial Networks (GANs), and Large Language Models (LLMs). It analyzes the application and achievements of these models in stock market forecasting. Moreover, the article also outlines the commonly used datasets and various evaluation metrics in the field of stock forecasting, further exploring unresolved issues and potential future research directions, aiming to provide clear guidance and reference for researchers in stock forecasting.},
	language = {en},
	urldate = {2025-11-03},
	journal = {Information Fusion},
	author = {Bao, Wuzhida and Cao, Yuting and Yang, Yin and Che, Hangjun and Huang, Junjian and Wen, Shiping},
	month = jan,
	year = {2025},
	note = {JCR分区: Q1
中科院分区升级版: 计算机科学1区
影响因子: 15.5
5年影响因子: 17.9
EI: 是},
	pages = {102616},
	annote = {关于数据驱动的股票预测，不同模型的对比分析综述
3.6节，基于transformer的预测模型

},
	file = {PDF:C\:\\Users\\lcy20\\Zotero\\storage\\XS869GXE\\Bao 等 - 2025 - Data-driven stock forecasting models based on neural networks A review.pdf:application/pdf},
}

@article{su_improved_2023,
	title = {An improved {BERT} method for the evolution of network public opinion of major infectious diseases: {Case} study of {COVID}-19},
	volume = {233},
	issn = {09574174},
	shorttitle = {An improved {BERT} method for the evolution of network public opinion of major infectious diseases},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417423014409},
	doi = {10.1016/j.eswa.2023.120938},
	abstract = {Infectious diseases can cause a sudden and serious spread of public opinion, making it a popular topic for analysis. However, current research on public sentiment mostly relies on traditional machine learning methods, which are limited by sample size and labor costs. This paper presents a novel deep learning model called PCABERT, an improved BERT model that utilizes principal component analysis (PCA) to extract and fuse the effective features of each layer of the BERT model. This approach offers a more accurate measurement of public sentiment. Furthermore, this paper proposes an analytical framework to comprehensively study the characteristics of network public opinion evolution in major infectious diseases from three perspectives: content, structure, and behavior. To validate the proposed model and framework, we analyze the COVID-19 pandemic as a case study and collect social media data from the past three years since the outbreak. We calculate public emotions using the PCA-BERT model and combine the obtained emotional values to summarize the temporal and spatial laws of the evolution of network public opinion in terms of content, structure, and behavior. This study can help guide the government to identify public demands during the epidemic and carry out epidemic prevention and control more effectively.},
	language = {en},
	urldate = {2025-11-03},
	journal = {Expert Systems with Applications},
	author = {Su, Meng and Cheng, Dongsheng and Xu, Yan and Weng, Futian},
	month = dec,
	year = {2023},
	note = {TLDR: A novel deep learning model called PCA-BERT, an improved BERT model that utilizes principal component analysis (PCA) to extract and fuse the effective features of each layer of the BERTmodel is presented, which can help guide the government to identify public demands during the epidemic and carry out epidemic prevention and control more effectively.
JCR分区: Q1
中科院分区升级版: 计算机科学1区
影响因子: 7.5
5年影响因子: 7.8
EI: 是},
	pages = {120938},
	annote = {PCA-BERT，用于融合层中信息
},
	file = {PDF:C\:\\Users\\lcy20\\Zotero\\storage\\2684C42J\\Su 等 - 2023 - An improved BERT method for the evolution of network public opinion of major infectious diseases Ca.pdf:application/pdf},
}

@article{kaur_bert-cnn_2023,
	title = {{BERT}-{CNN}: {Improving} {BERT} for requirements classification using {CNN}},
	volume = {218},
	issn = {18770509},
	shorttitle = {{BERT}-{CNN}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S187705092300234X},
	doi = {10.1016/j.procs.2023.01.234},
	abstract = {Requirements classification is considered a crucial task in requirements engineering. The analysis of functional and Nonfunctional requirements (NFRs) requires domain knowledge. NFRs are considered quality attributes, that hold critical information about the constraints upon which the success of software depends. Usually, requirements are expressed in natural language, and extraction and classification of such requirements become a challenging task. Various automatic techniques for requirements classification are exploited by existing studies. Recently, Transfer learning as a new deep learning model attracts the attention of researchers, which excelled in different Natural Language Processing tasks. Transfer learning-based BERT pretrained model achieved more promising results than state-of-the-art approaches. This proposed research presents a Bidirectional Encoder-Decoder Transformer-Convolutional Neural Network (BERT-CNN) model for requirements classification. Then, the convolutional layer is stacked over the BERT layer for performance enhancement. This research work conducted an experiment on the PROMISE dataset of 625 requirements. The experiment results demonstrate that the proposed model performs better than the state-of-the-art baseline approach.},
	language = {en},
	urldate = {2025-11-03},
	journal = {Procedia Computer Science},
	author = {Kaur, Kamaljit and Kaur, Parminder},
	year = {2023},
	note = {EI: 是},
	keywords = {ccfInfo: Not Found, citationNumber: 7},
	pages = {2604--2611},
	annote = {2020年于会议提出BERT-CNN架构
},
	file = {PDF:C\:\\Users\\lcy20\\Zotero\\storage\\YB6H4F9H\\Kaur和Kaur - 2023 - BERT-CNN Improving BERT for requirements classification using CNN.pdf:application/pdf},
}

@article{min_recent_2024,
	title = {Recent advances in natural language processing via large pre-trained language models: {A} survey},
	volume = {56},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Recent {Advances} in {Natural} {Language} {Processing} via {Large} {Pre}-trained {Language} {Models}},
	url = {https://dl.acm.org/doi/10.1145/3605943},
	doi = {10.1145/3605943},
	abstract = {Large, pre-trained language models (PLMs) such as BERT and GPT have drastically changed the Natural Language Processing (NLP) field. For numerous NLP tasks, approaches leveraging PLMs have achieved state-of-the-art performance. The key idea is to learn a generic, latent representation of language from a generic task once, then share it across disparate NLP tasks. Language modeling serves as the generic task, one with abundant self-supervised text available for extensive training. This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques. It surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches. In addition, it discusses PLM limitations and suggested directions for future research.},
	language = {en},
	number = {2},
	urldate = {2025-11-04},
	journal = {ACM Computing Surveys},
	author = {Min, Bonan and Ross, Hayley and Sulem, Elior and Veyseh, Amir Pouran Ben and Nguyen, Thien Huu and Sainz, Oscar and Agirre, Eneko and Heintz, Ilana and Roth, Dan},
	month = feb,
	year = {2024},
	note = {TLDR: This article presents the key fundamental concepts of PLM architectures and a comprehensive view of the shift to PLM-driven NLP techniques, and surveys work applying the pre-training then fine-tuning, prompting, and text generation approaches.
JCR分区: Q1
中科院分区升级版: 计算机科学1区
影响因子: 28.0
5年影响因子: 26.3
EI: 是
中科院升级版Top分区: 计算机科学TOP},
	keywords = {ccfInfo: CCF-None CSUR, citationNumber: 1727},
	pages = {1--40},
	annote = {NLP领域综述
},
	file = {PDF:C\:\\Users\\lcy20\\Zotero\\storage\\74VXJFT7\\Min 等 - 2024 - Recent advances in natural language processing via large pre-trained language models A survey.pdf:application/pdf},
}

@article{rogers_primer_2020,
	title = {A primer in {BERTology}: {What} we know about how {BERT} works},
	volume = {8},
	issn = {2307-387X},
	shorttitle = {A {Primer} in {BERTology}},
	url = {https://direct.mit.edu/tacl/article/96482},
	doi = {10.1162/tacl_a_00349},
	abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.},
	language = {en},
	urldate = {2025-11-04},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	month = dec,
	year = {2020},
	note = {TLDR: This paper is the first survey of over 150 studies of the popular BERT model, reviewing the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression.
JCR分区: Q1
中科院分区升级版: 计算机科学2区
影响因子: 6.9
5年影响因子: 10.7
EI: 是
CCF: B},
	keywords = {ccfInfo: CCF-B TACL, citationNumber: 2218},
	pages = {842--866},
	annote = {文中明确提到，目前已经提出了中间层深层信息的需要被利用，但提到的研究只有类似加权融合
},
	file = {PDF:C\:\\Users\\lcy20\\Zotero\\storage\\7VJJGYBV\\Rogers 等 - 2020 - A primer in BERTology What we know about how BERT works.pdf:application/pdf},
}

@inproceedings{li_automatic_2019,
	title = {The automatic text classification method based on {BERT} and feature union},
	url = {https://ieeexplore.ieee.org/abstract/document/8975793},
	doi = {10.1109/ICPADS47876.2019.00114},
	abstract = {For the traditional model based on the deep learning method most used CNN(convolutional neural networks) or RNN(Recurrent neural Network) model and is based on the dynamic character-level embedding or word-level embedding as input, so there is a problem that the text feature extraction is not comprehensive. In the development environment of the Internet of Things, A method of Automatic text classification based on BERT(Bidirectional Encoder Representations from Transformers) and Feature Fusion was proposed in this paper. Firstly, the text-to-dynamic character-level embedding is transformed by the BERT model, and the BiLSTM(Bi-directional Long-Short Term Memory) and CNN output features are combined and merged to make full use of CNN to extract the advantages of local features and to use BiLSTM to have the advantage of memory to link the extracted context features to better represent the text, so as to improve the accuracy of text classification task. A comparative study with state-of-the-art approaches manifests the proposed method outperforms the state-of-the-art methods in accuracy. It can effectively improve the accuracy of tag prediction for text data with sequence features and obvious local features.},
	urldate = {2025-11-04},
	booktitle = {2019 {IEEE} 25th {International} {Conference} on {Parallel} and {Distributed} {Systems} ({ICPADS})},
	author = {Li, Wenting and Gao, Shangbing and Zhou, Hong and Huang, Zihe and Zhang, Kewen and Li, Wei},
	month = dec,
	year = {2019},
	note = {ISSN: 1521-9097
TLDR: The proposed method of Automatic text classification based on BERT(Bidirectional Encoder Representations from Transformers) and Feature Fusion outperforms the state-of-the-art methods in accuracy and can effectively improve the accuracy of tag prediction for text data with sequence features and obvious local features.},
	keywords = {Accuracy, Bidirectional control, Bidirectional long short term memory, ccfInfo: CCF-C ICPADS, citationNumber: 43, Convolutional neural networks, Encoding, Feature extraction, Heat maps, Neural networks, NLP, BERT, BiLSTM, CNN, Feature Union, Text classification, Text categorization, Transformers},
	pages = {774--777},
	annote = {首个关于CNN等与BERT结合的论文
},
	file = {PDF:C\:\\Users\\lcy20\\Zotero\\storage\\ZRDVREU9\\Li - The automatic text classification method based on BERT and feature union.pdf:application/pdf},
}

@article{zhang_bert-jam_2021,
	title = {{BERT}-{JAM}: {Maximizing} the utilization of {BERT} for neural machine translation},
	volume = {460},
	shorttitle = {{BERT}-{JAM}},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231221010365},
	urldate = {2025-11-04},
	journal = {Neurocomputing},
	author = {Zhang, Zhebin and Wu, Sai and Jiang, Dawei and Chen, Gang},
	year = {2021},
	note = {Publisher: Elsevier
JCR分区: Q1
中科院分区升级版: 计算机科学2区
影响因子: 6.5
5年影响因子: 6.0
EI: 是
CCF: C},
	keywords = {ccfInfo: CCF-C, citationNumber: 18},
	pages = {84--94},
	annote = {明确提到，只观察最后一层的量会导致信息流失，但只对层进行了简单融合。
},
	file = {1-s2.0-S0925231221010365-main:C\:\\Users\\lcy20\\Zotero\\storage\\Q2US7HVV\\1-s2.0-S0925231221010365-main.pdf:application/pdf},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of deep bidirectional transformers for language understanding},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423/},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2025-11-04},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	month = jun,
	year = {2019},
	note = {TLDR: A new language representation model, BERT, designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks.},
	keywords = {ccfInfo: CCF-B NAACL, citationNumber: 148398},
	pages = {4171--4186},
	annote = {2019-BERT开山之作
},
	file = {Full Text PDF:C\:\\Users\\lcy20\\Zotero\\storage\\F5BJG8DT\\Devlin 等 - 2019 - BERT Pre-training of deep bidirectional transformers for language understanding.pdf:application/pdf},
}
